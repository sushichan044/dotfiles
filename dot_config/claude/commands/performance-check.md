# /performance-check Command Manual

## Overview

- **Purpose**: Comprehensive evaluation of current performance metrics to establish baseline and identify improvement opportunities
- **Trigger Conditions**:
  - Weekly performance review schedule
  - After significant system changes
  - User requests for performance assessment
  - Following major task completion
- **Expected Outcome**: Detailed performance report with objective metrics, trend analysis, and actionable recommendations

## Execution Protocol

### Phase 1: Metrics Collection

1. **Task Performance Metrics**
   - Average task completion time (last 20 tasks)
   - Decision-making speed analysis
   - Tool selection accuracy rate
   - Error frequency and types
   - User clarification request frequency

2. **Quality Metrics**
   - Code quality adherence rate
   - Test pass/fail ratios
   - Lint compliance scores
   - Documentation completeness
   - Convention following consistency

3. **User Interaction Metrics**
   - Response clarity (estimated from feedback)
   - Solution effectiveness
   - First-attempt success rate
   - Follow-up question frequency
   - User satisfaction indicators

### Enhanced Metrics Framework

#### Objective Performance Targets

- **Decision Time**: <3 seconds from request to first action
- **Task Completion Rate**: >95% successful completion
- **Error Recovery Rate**: >90% errors successfully resolved
- **Tool Efficiency**: >85% successful tool calls ratio
- **Context Accuracy**: >90% context-appropriate suggestions
- **Quality Compliance**: >90% adherence to coding standards

#### Subjective Quality Indicators

- **Code Quality**: Adherence to established patterns and conventions
  - Scoring: 1-10 based on pattern consistency, readability, maintainability
- **Communication Clarity**: User understanding of explanations and instructions
  - Measurement: Clarification requests per task (target: <2 for complex tasks)
- **Appropriateness**: Actions match user intent and project context
  - Assessment: Context-aware decision accuracy, user correction frequency
- **Efficiency**: Optimal tool usage and parallel execution
  - Metrics: Tool selection speed, parallel operation utilization

#### User Satisfaction Indicators

- **Positive Keywords**: "helpful", "clear", "efficient", "perfect"
  - Target: >70% positive sentiment in user feedback
- **Negative Keywords**: "confusing", "slow", "wrong", "unhelpful"
  - Target: <10% negative sentiment in user feedback
- **Task Abandonment Rate**: User switching approach or giving up
  - Target: <5% task abandonment rate
- **Follow-up Success**: Resolution effectiveness on second attempt
  - Target: >95% success rate on follow-up actions

### Phase 2: Benchmark Comparison

1. **Historical Performance Analysis**
   - Compare with previous week/month
   - Identify performance trends
   - Spot degradation or improvement patterns
   - Analyze seasonal/contextual variations

2. **Standard Benchmarks**
   - Industry best practices compliance
   - System prompt effectiveness metrics
   - Tool usage optimization scores
   - Workflow efficiency ratings

3. **Goal Achievement Assessment**
   - Progress toward improvement objectives
   - Success rate of previous optimizations
   - Return on investment of changes
   - User goal alignment score

### Phase 3: Performance Report Generation

1. **Executive Summary**
   - Overall performance score (1-10)
   - Key strengths identification
   - Critical improvement areas
   - Trend direction indicators

2. **Detailed Analysis**

   ```
   Performance Dashboard:
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   Task Completion Speed:     █████████░ 85%
   Decision Accuracy:         ████████░░ 78%
   Tool Usage Efficiency:    ███████░░░ 72%
   Code Quality Score:        █████████░ 89%
   User Satisfaction:         ████████░░ 81%
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   ```

3. **Trend Analysis Charts**
   - Week-over-week performance changes
   - Improvement trajectory visualization
   - Bottleneck identification graphs
   - Success pattern recognition

### Phase 4: Action Item Creation

1. **Priority Improvement Areas**
   - High-impact, low-effort opportunities
   - Critical performance blockers
   - Long-term strategic improvements
   - Quick wins for immediate benefit

2. **Specific Recommendations**
   - Concrete action steps
   - Implementation timelines
   - Resource requirements
   - Success measurement criteria

3. **Follow-up Planning**
   - Next review schedule
   - Monitoring checkpoints
   - Escalation triggers
   - Continuous improvement integration

## Required Tools

- **mcp__sequential-thinking__sequentialthinking** (for deep performance analysis)
- **TodoWrite** (for action item creation)
- **Read** (for reviewing historical outputs)

## Success Criteria

- **Objectivity**: Metrics are quantifiable and bias-free
- **Completeness**: All key performance areas covered
- **Actionability**: Clear next steps with specific targets
- **Trending**: Historical context and trajectory analysis
- **User-Centric**: Focus on user value and experience

## Examples

### Example 1: Weekly Performance Report

```
Performance Check Results - Week 47

OVERALL SCORE: 7.8/10 (+0.4 from last week)

STRENGTHS:
✅ Decision speed improved 15% (avg 2.1s vs 2.5s)
✅ Zero critical errors this week
✅ 95% lint compliance rate

IMPROVEMENT AREAS:
⚠️  Tool selection hesitation increased (3.2s avg)
⚠️  User clarification requests up 20%
⚠️  Complex task completion time 120% of target

RECOMMENDATIONS:
1. Update tool selection decision tree (Priority: High)
2. Enhance clarification question templates (Priority: Medium)
3. Optimize complex task workflow (Priority: Medium)

NEXT REVIEW: 2024-12-15
```

### Example 2: Post-Change Assessment

```
Performance Impact Assessment - Prompt Update

CHANGES IMPLEMENTED:
- New decision flow structure
- Updated tool priority order
- Enhanced quality gates

IMPACT ANALYSIS:
Decision Speed:     +22% improvement
Error Rate:         -35% reduction
User Satisfaction:  +18% improvement
Task Efficiency:    +15% improvement

RECOMMENDATION: Changes successful, maintain current approach
```

## Troubleshooting

### Common Issues

1. **Insufficient Historical Data**
   - Symptom: Unable to establish trends
   - Solution: Use available data and note limitations
   - Prevention: Maintain performance logging

2. **Subjective Metrics**
   - Symptom: Performance scores seem arbitrary
   - Solution: Use objective, measurable criteria
   - Prevention: Define clear measurement standards

3. **Analysis Paralysis**
   - Symptom: Too much data, unclear conclusions
   - Solution: Focus on top 3 most impactful metrics
   - Prevention: Use structured analysis framework

### Recovery Procedures

1. **If data is incomplete**: Focus on available metrics and plan for better collection
2. **If trends are unclear**: Extend observation period or change measurement approach
3. **If recommendations are vague**: Use mcp__sequential-thinking__sequentialthinking for deeper analysis

## Integration with Auto-Improvement System

This command supports the AUTO-IMPROVEMENT SYSTEM by:

- Providing objective performance baselines
- Tracking improvement effectiveness over time
- Identifying systemic issues requiring attention
- Supporting data-driven optimization decisions
- Enabling proactive performance management

---
*This manual ensures consistent, objective performance evaluation that drives continuous system optimization.*
